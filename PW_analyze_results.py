from matplotlib import pyplot as plt
import numpy as np
import linecache
import shutil
import pickle
import scipy
import nrrd
import yaml
import pdb
import os

import tensorflow as tf
import patch_utils
import PW_NNAL
import PW_NN
import PW_AL
import NN



def get_queries(expr, run, method_name):
    """Simply giving back the queries generated
    in different queries separately
    """
    
    Qs = []

    Q_dir = os.path.join(
        expr.root_dir,str(run),
        method_name, 'queries')
    Q_files = os.listdir(Q_dir)
    for f in Q_files:
        fullpath = os.path.join(
            Q_dir, f)
        Qs += [np.int32(np.loadtxt(
            fullpath))]
        
    return Qs

def get_queries_type(expr,run,method_name):
    """Getting type of queries generated by
    a specific method of a run of AL experiment
    """
    
    stypes = []
    Qs = get_queries(expr, run, method_name)
    
    Q_types = []
    for Q in Qs:
        t = get_sample_type(
            expr, run, Q)
        Q_types += [t]
        
    return Q_types


def get_sample_type(expr, run, inds):
    """Getting type of a given set of
    indexed samples in a run of an
    experiment
    """

    stypes = []
    for ind in inds:
        line = linecache.getline(
            os.path.join(
                expr.root_dir,
                str(run),
                'inds.txt'), ind)
        stypes += [int(line.splitlines(
        )[0].split(',')[-1])]

    return stypes

def get_slice_preds(expr,
                    run,
                    model,
                    inds,
                    slice_,
                    sess):
    """Getting the results of 
    class prediction of a set of indexed
    voxels in a given slices of the 
    image
    """
    
    # take only indices of the 
    # given slice
    if expr.pars['data']=='adults':
        img_addrs, mask_addrs = patch_utils.extract_Hakims_data_path()
    elif expr.pars['data']=='newborn':
        img_addrs, mask_addrs = patch_utils.extract_newborn_data_path()

    img_addr = img_addrs[expr.pars[
        'indiv_img_ind']]
    img,_ = nrrd.read(img_addr)

    inds_path = os.path.join(expr.root_dir,
                             str(run),
                             'inds.txt')
    samples_dict,_ = PW_AL.create_dict(
        inds_path, inds)
    multinds = np.unravel_index(
        samples_dict[img_addr], 
        img.shape)
    slice_indics = multinds[2]==slice_
    slice_multinds = (
        multinds[0][slice_indics],
        multinds[1][slice_indics])
    slice_inds = inds[slice_indics]
    
    # prediction
    preds = PW_AL.batch_eval_winds(
        expr,
        run,
        model,
        slice_inds,
        'prediction',
        sess)
    
    return preds, slice_multinds


def visualize_eval_metrics(expr,
                           run,
                           metric,
                           methods=[],
                           colors=[]):
    """Visualize performance evaluations
    of a set of methods in an experiment's
    run

    Size of the color vector `colors` should
    be the number of included path plus
    one (if there also exists the performance
    metric value for the full pool data set)
    """

    run_path = os.path.join(expr.root_dir,
                            str(run))
    if len(methods)==0:
        methods = [f for f in os.listdir(run_path) 
                   if os.path.isdir(os.path.join(
                           run_path, f))]

    # maximum number of queries among methods
    M = 0
    for i, method_name in enumerate(methods):
        if metric=='F1':
            # vector of evaluation metrics
            F = np.loadtxt(os.path.join(
                run_path, 
                method_name,
                'perf_evals.txt'))
        elif metric=='Precision':
            F = get_eval_metrics(
                expr, run, method_name)[0,:]
        elif metric=='Recall':
            F = get_eval_metrics(
                expr, run, method_name)[1,:]


        # vector of numbre of observed
        # labels at each query iterations
        Qset = get_queries(expr, run, 
                           method_name)
        Qsizes = [0] + [len(Q) for Q in Qset]
        Qsizes = np.cumsum(Qsizes)

        # if the last iteration is still not
        # evaluated, ignore the queries
        if len(Qsizes)==len(F)+1:
            Qsizes = Qsizes[:-1]

        M = max(M, Qsizes[-1])
        # plotting this curve
        if len(colors)>0:
            plt.plot(Qsizes, F, 
                     linewidth=2,
                     color=colors[i],
                     marker = '*',
                     label=method_name)
        else:
            plt.plot(Qsizes, F, 
                     linewidth=2,
                     marker='*',
                     label=method_name)

    # get the full performance 
    if os.path.exists(os.path.join(
            run_path, 
            'pooltrain_eval.txt')):
        full_F = np.loadtxt(os.path.join(
            run_path, 
            'pooltrain_eval.txt'))

        if len(colors)>0:
            plt.plot([0,M], 
                     [full_F, full_F],
                     linewidth=2,
                     color=colors[-1],
                     label='Pool-training')
        else:
            plt.plot([0,M], 
                     [full_F, full_F], 
                     linewidth=2,
                     label='Pool-training')

    plt.legend(fontsize=15)
    plt.xlabel('# Queries', fontsize=15)
    plt.ylabel(metric, fontsize=15)
    plt.grid()

def get_preds_stats(preds, mask):
    """Computing different statistics of
    a set of prediction in comparison with
    the ground truth labels, such as P, N, 
    TP, TN, FP, FN
    
    At this time, this function deals only
    with single images (and not a dictionary
    of multiple images). That is to say, the
    inputs are two arrays of the same size, 
    and with binary values (0 or 1)
    """

    P = float(np.sum(mask>0))
    N = float(np.sum(mask==0))
    TP = float(np.sum(np.logical_and(
        preds>0, mask>0)))
    FP = float(np.sum(np.logical_and(
        preds>0, mask==0)))
    TN = float(np.sum(np.logical_and(
        preds==0, mask==0)))
    FN = float(np.sum(np.logical_and(
        preds==0, mask>0)))

    return P, N, TP, FP, TN, FN
    

def get_Fmeasure(preds, mask):
    
    # computing total TPs, Ps, and
    # TPFPs (all positives)
    P  = 0
    TP = 0
    TPFP = 0
    if isinstance(preds, dict):
        for img_path in list(preds.keys()):
            ipreds = preds[img_path]
            imask = np.array(mask[img_path])
            
            P  += np.sum(imask>0)
            TP += np.sum(np.logical_and(
                ipreds>0, imask>0))
            TPFP += np.sum(ipreds>0)
    else:
        
        P  += np.sum(mask>0)
        TP += np.sum(np.logical_and(
            preds>0, mask>0))
        TPFP += np.sum(preds>0)

    # precision and recall
    Pr = TP / TPFP
    Rc = TP / P
    
    # F measure
    return 2/(1/Pr + 1/Rc)

def get_eval_metrics(expr,
                     run,
                     method_name):
    """Computing different evaluation
    metrics of the predictions in results
    of running a specific querying
    method in an experiment's run
    """

    run_path = os.path.join(
        expr.root_dir, str(run))

    # load ground truth labels
    labels_path = os.path.join(
        run_path, 'labels.txt')
    test_lines = np.int64(np.loadtxt(
        os.path.join(run_path, 
                     'test_lines.txt')))
    test_labels = PW_AL.read_label_lines(
        labels_path, test_lines)

    # load predictions
    preds_path = os.path.join(
        run_path, 
        method_name, 
        'predicts.txt')
    preds = np.loadtxt(preds_path)
    
    iter_cnt = preds.shape[0]
    Metrs = np.zeros((2, iter_cnt))
    for i in range(iter_cnt):
        (P, N, TP, 
         FP, TN, FN) = get_preds_stats(
             preds[i,:], test_labels)
        
        # Precision
        Metrs[0,i] = TP / (TP+FP)
        # Recall
        Metrs[1,i] = TP / P

    return Metrs
    
    
        
        
