{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import listdir, sys\n",
    "from os.path import isfile, join\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import time\n",
    "from imp import reload\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_file_path = \"/home/ch194765/repos/atlas-active-learning/\"\n",
    "\n",
    "sys.path.insert(0, read_file_path)\n",
    "import prep_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with TF Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input\n",
    "X = tf.placeholder(\"float\", [3, None])\n",
    "y = tf.placeholder(\"float\", [2, None])\n",
    "\n",
    "# layer one\n",
    "W_1 = tf.Variable(tf.ones([3,3]))\n",
    "b_1 = tf.Variable(tf.zeros([3,1]))\n",
    "z_1 = tf.add(tf.matmul(W_1, X), b_1)\n",
    "a_1 = tf.nn.sigmoid(z_1)\n",
    "\n",
    "# layer two (last layer)\n",
    "W_2 = tf.Variable(tf.ones([2,3]))\n",
    "b_2 = tf.Variable(tf.zeros([2,1]))\n",
    "z_2 = tf.add(tf.matmul(W_2, a_1), b_2)\n",
    "a_2 = tf.sigmoid(z_2)   # (output)\n",
    "\n",
    "# loss function\n",
    "J = tf.multiply(tf.reduce_sum(tf.pow(tf.subtract(y, a_2), 2)), .5)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.014189,  0.014189,  0.      ],\n",
       "        [ 0.014189,  0.014189,  0.      ],\n",
       "        [ 0.014189,  0.014189,  0.      ]], dtype=float32),\n",
       " array([[ 0.02837801],\n",
       "        [ 0.02837801],\n",
       "        [ 0.02837801]], dtype=float32),\n",
       " array([[ 0.05275872,  0.05275872,  0.05275872],\n",
       "        [ 0.05275872,  0.05275872,  0.05275872]], dtype=float32),\n",
       " array([[ 0.07216756],\n",
       "        [ 0.07216756]], dtype=float32)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xin = np.array([[1., 0.], [0., 1.], [0., 0.]])\n",
    "yin = np.array([[1., 0.], [0., 1.]])\n",
    "grad_vals = tf.gradients(J, [W_1, b_1, W_2, b_2])\n",
    "sess.run(grad_vals, feed_dict={X:Xin, y:yin})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(56)\n",
    "G = optimizer.compute_gradients(J, [W_1, b_1, W_2, b_2])\n",
    "Gs = sess.run(G, feed_dict={X:Xin, y:yin})\n",
    "#Gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Some Active Learning on an MNIST Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load the data from TF \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we don't use all the training data of the `mnist` object since we need to leave some part of the training unlabeled to do querying. Hence, we manually take a portion of the training as the initial labeled data set, and use the rest for evaluating our active learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of the initial labeled data set\n",
    "init_size = 200\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "\n",
    "# randomly selecting that many samples from the training data set\n",
    "train_size = mnist.train.images.shape[0]\n",
    "rand_inds = np.random.permutation(train_size)\n",
    "init_inds = rand_inds[:init_size]\n",
    "unlabeled_inds = rand_inds[init_size:]\n",
    "#\n",
    "init_train_images = mnist.train.images[init_inds, :]\n",
    "init_train_labels = mnist.train.labels[init_inds, :]\n",
    "pool_images = mnist.train.images[unlabeled_inds, :]\n",
    "pool_labels = mnist.train.labels[unlabeled_inds, :]\n",
    "\n",
    "# manually creating batches for initial training\n",
    "batch_inds = prep_dat.gen_batch_inds(init_size, batch_size)\n",
    "batch_of_data = prep_dat.gen_batch_matrices(init_train_images, batch_inds)\n",
    "batch_of_labels = prep_dat.gen_batch_matrices(init_train_labels, batch_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the initial training data set to train a simple NN over the data using cross-entropy or any other loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# input and output placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "# parameters\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# output should be a normalized \n",
    "y = tf.matmul(x,W) + b\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "# optimization iteration\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# doing the optimization\n",
    "for _ in range(epochs):\n",
    "    for i in range(len(batch_inds)):\n",
    "      train_step.run(feed_dict={x: batch_of_data[i], \n",
    "                                y_: batch_of_labels[i]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracy: 0.7894\n"
     ]
    }
   ],
   "source": [
    "# Evaluation:\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Initial Accuracy: %.4f\" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# log-posteriors (or log-likelihood wrt the parameters)\n",
    "log_posteriors = tf.log(tf.nn.softmax(y))\n",
    "\n",
    "# compute the gradiants for all the tests\n",
    "test_size = mnist.test.images.shape[0]\n",
    "c = mnist.test.labels.shape[1]\n",
    "gr_log_posteriors = np.zeros((test_size, c))\n",
    "for i in range(test_size):\n",
    "    for j in range(c):\n",
    "        grads = tf.gradients(log_posteriors[0][j], [W, b])\n",
    "        # we use [i:i+1,:] since if we only use [i,:] TF gets upset\n",
    "        grW, grb = sess.run(grads, feed_dict={x: mnist.test.images[i:i+1,:], y_: mnist.test.labels[i:i+1,:]})\n",
    "        gr_log_posteriors[i,j] = np.sum(grW**2) + np.sum(grb**2)\n",
    "    \n",
    "    #if i % 100 == 0: print(i)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=10\n",
    "log_posteriors = tf.log(tf.nn.softmax(y))\n",
    "#sess.run(log_posteriors[0,i], feed_dict={x: mnist.test.images[:1,:], y_: mnist.test.labels[:1,:]})\n",
    "grW, grb = sess.run(grads, feed_dict={x: mnist.test.images[i:i+1,:], y_: mnist.test.labels[i:i+1,:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         ..., \n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32),\n",
       "  array([ 0.96572477,  0.99484724,  0.95568514,  0.95616174,  0.89419389,\n",
       "          0.97412902,  0.99826747, -8.42816162,  0.95308042,  0.73607177], dtype=float32)]]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_posteriors = tf.log(tf.nn.softmax(y))\n",
    "#grads = tf.gradients(log_posteriors, [W, b])\n",
    "costs = np.array([log_posteriors[0,:]])\n",
    "grads = [tf.gradients(cost, [W,b]) for cost in costs]\n",
    "sess.run(grads, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "#grads = [tf.gradients(cost) for cost in log_posteriors]\n",
    "#grW, grb = sess.run(grads, feed_dict={x: mnist.test.images, y_: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = np.empty((test_size, c), dtype=object)\n",
    "for i in range(test_size):\n",
    "    for j in range(c):\n",
    "        costs[i,j] = log_posteriors[i,j]\n",
    "    if i % 100==0: print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.stack(log_posteriors[i,j] in zip(np.meshgrid(np.arange(20),np.arange(c))))\n",
    "#tf.stack([log_posteriors[j,i] for (j,i) in (np.arange(10),np.arange(10))])\n",
    "#costs = log_posteriors[0,:]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
